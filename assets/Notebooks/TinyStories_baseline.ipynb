{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yellowssnake/221_zlotin/blob/main/assets/Notebooks/TinyStories_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4t13JUZC-Ok",
        "outputId": "743ea81d-81d8-41c4-fa01-ad21e64e3671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets accelerate nvidia-ml-py3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description\n",
        "\n",
        "In this assignment, you will train a language model (LM) using the TinyStories dataset, focusing on optimizing model performance within the constraints of Google Colab’s hardware. For the sake of speed, we will do it on the part of the dataset.\n",
        "\n",
        "```\n",
        "Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun.\n",
        "Beep was a healthy car because he always had good fuel....\n",
        "```\n",
        "\n",
        "Your objective is to maximize the size of the model without exceeding the available computational resources (~ 16GB VRAM). You could start with the Hugging Face Transformers library and experiment with various memory optimization techniques, such as (but not limited to):\n",
        "\n",
        " * Different batch size\n",
        " * Different optimizer\n",
        " * Gradient accumulation\n",
        " * Activation checkpointing\n",
        " * CPU offloading\n",
        " * 8bit optimizers\n",
        "\n",
        "You have a baseline of training gpt-2 model prepared in this  colab notebook. You can easily switch it to opt-350m, opt-1.3b, gpt2 etc. You can find a great beginner-level guide on the topic [here](https://huggingface.co/docs/transformers/v4.18.0/en/performance).\n",
        "\n",
        "```\n",
        "A long time ago in a galaxy far far away... a little girl named Lily was playing in the garden. She was so excited! She wanted to explore the garden and see what was around her.\n",
        "Suddenly, she heard a loud noise. Lily looked up and saw a big, hairy creature. Lily was so excited! She ran to the creature and grabbed it by the arm. The creature was so big and hairy that Lily couldn't help but laugh.\n",
        "```\n",
        "\n",
        "![](https://hse24.fmin.xyz/gpt2_generation.jpeg)\n",
        "\n",
        "You have to fill this table with your description/observations.\n",
        "\n",
        "| Setup | # of parameters | GPU peak memory, MB | Final eval loss | Batch Size | Time to run 5 epochs, s | Generation example | Comment |\n",
        "|:---:|:---:|:---:|:---:|:---:|:---:|:---------:|:---------:|\n",
        "| Baseline (OPT-125M) | 125 M | 9044 | 1.928 | 8 | 442.34 | `A long time ago in a galaxy far far away... there was a little girl named Lily. She was three years old and loved to explore. One day, she decided to go for a walk in the park. Lily was so excited to go for a walk. She asked her mom, \"What do you want to do?\" Her mom smiled and said, \"I want to explore the galaxy.\" Lily was so excited to explore the galaxy.` |  |\n",
        "| Baseline (GPT2-S) | 124 M | 13016 | 2.001 | 8 | 487.75 | `A long time ago in a galaxy far far away... a little girl named Lily was playing in the garden. She was so excited! She wanted to explore the garden and see what was around her. Suddenly, she heard a loud noise. Lily looked up and saw a big, hairy creature. Lily was so excited! She ran to the creature and grabbed it by the arm. The creature was so big and hairy that Lily couldn't help but laugh.` | The generation seems more interesting, despite the fact, that eval loss is higher. |\n",
        "|  |  |  |  |  |  |  |  |\n",
        "|  |  |  |  |  |  |  |  |\n",
        "|  |  |  |  |  |  |  |  |\n",
        "|  |  |  |  |  |  |  |  |\n",
        "\n",
        "For each unique trick for memory optimization, you will get 4 points (maximum 20 points). A combination of tricks is not counted as a unique trick, but will, probably, be necessary to train big models. The maximum grade is bounded with the size of the trained model:\n",
        "\n",
        "* If the model size you train is <= 125M - you can get a maximum of 8 points.\n",
        "* If the model size you train is 126M <= 350M - you can get a maximum of 12 points.\n",
        "* If the model size you train is 350M <= 1B - you can get a maximum of 16 points.\n",
        "* If you fit 1B model or more - you can get a maximum 20 points."
      ],
      "metadata": {
        "id": "__D14ZpGPAwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline"
      ],
      "metadata": {
        "id": "sdGM0iLC5NVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "torch.cuda.empty_cache()  # Clears the cache\n",
        "torch.cuda.reset_peak_memory_stats()  #\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, \\\n",
        "    TrainingArguments, Trainer, logging, DataCollatorForLanguageModeling\n",
        "from datasets import load_datasetsx\n",
        "from pynvml import *\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "print(\"💎 Before training:\")\n",
        "print_gpu_utilization()\n",
        "\n",
        "# Suppress less critical logs\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Load the dataset with both training and evaluation splits\n",
        "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:500]\")\n",
        "eval_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[500:1000]\")\n",
        "\n",
        "HF_cardname = \"openai-community/gpt2\"\n",
        "# HF_cardname = \"openai-community/gpt2-medium\"\n",
        "# HF_cardname = \"openai-community/gpt2-large\"\n",
        "# HF_cardname = \"openai-community/gpt2-XL\"\n",
        "# HF_cardname = \"facebook/opt-125m\"\n",
        "# HF_cardname = \"facebook/opt-350m\"\n",
        "# HF_cardname = \"facebook/opt-1.3b\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_cardname, use_fast=False)\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Ensure the tokenizer has a padding token, set EOS_TOKEN as padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = EOS_TOKEN\n",
        "\n",
        "# Function to process the dataset\n",
        "def formatting_func(examples):\n",
        "    inputs = [tokenizer(text + EOS_TOKEN, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\") for text in examples['text']]\n",
        "    return {'input_ids': [input['input_ids'].squeeze() for input in inputs], 'labels': [input['input_ids'].squeeze() for input in inputs]}\n",
        "\n",
        "# Process the datasets\n",
        "processed_train_dataset = train_dataset.map(formatting_func, batched=True, remove_columns=[\"text\"])\n",
        "processed_eval_dataset = eval_dataset.map(formatting_func, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Initialize Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "print(\"💎 Dataset loaded\")\n",
        "print_gpu_utilization()\n",
        "\n",
        "# Define and load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(HF_cardname)\n",
        "\n",
        "print(\"💎 Model loaded\")\n",
        "print_gpu_utilization()\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Initialize the trainer with the data collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_train_dataset,\n",
        "    eval_dataset=processed_eval_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "def print_summary(trainer):\n",
        "    # Access training result metrics directly from the trainer state\n",
        "    print(f\"💎 Training time: {trainer.state.log_history[-1]['train_runtime']:.2f} seconds\")\n",
        "    print(f\"Samples/second: {trainer.state.log_history[-1]['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()\n",
        "    num_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
        "    print(f\"Total Trainable Parameters: {num_params}\")\n",
        "\n",
        "print_summary(trainer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-UaswgYD6Ra",
        "outputId": "05e39882-45f5-4a38-cf45-0df54f2b0131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💎 Before training:\n",
            "GPU memory occupied: 1978 MB.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💎 Dataset loaded\n",
            "GPU memory occupied: 1978 MB.\n",
            "💎 Model loaded\n",
            "GPU memory occupied: 1978 MB.\n",
            "{'eval_loss': 2.0613200664520264, 'eval_runtime': 24.2955, 'eval_samples_per_second': 20.58, 'eval_steps_per_second': 2.593, 'epoch': 1.0}\n",
            "{'eval_loss': 2.0225725173950195, 'eval_runtime': 24.3054, 'eval_samples_per_second': 20.572, 'eval_steps_per_second': 2.592, 'epoch': 2.0}\n",
            "{'eval_loss': 2.006747007369995, 'eval_runtime': 24.1332, 'eval_samples_per_second': 20.718, 'eval_steps_per_second': 2.611, 'epoch': 3.0}\n",
            "{'eval_loss': 2.0053534507751465, 'eval_runtime': 24.5421, 'eval_samples_per_second': 20.373, 'eval_steps_per_second': 2.567, 'epoch': 4.0}\n",
            "{'eval_loss': 2.0014288425445557, 'eval_runtime': 24.1794, 'eval_samples_per_second': 20.679, 'eval_steps_per_second': 2.606, 'epoch': 5.0}\n",
            "{'train_runtime': 487.7478, 'train_samples_per_second': 5.126, 'train_steps_per_second': 0.646, 'train_loss': 2.1369303385416667, 'epoch': 5.0}\n",
            "💎 Training time: 487.75 seconds\n",
            "Samples/second: 5.13\n",
            "GPU memory occupied: 13016 MB.\n",
            "Total Trainable Parameters: 124439808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the prompt text\n",
        "input_ids = tokenizer.encode(\n",
        "    \"A long time ago in a galaxy far far away...\",\n",
        "    return_tensors=\"pt\").cuda()\n",
        "\n",
        "# Generate text using the model\n",
        "output_ids = model.generate(input_ids, max_length=100)\n",
        "\n",
        "# Decode the generated ids to text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfelptagsJpN",
        "outputId": "ce9745c1-af0d-4324-c60c-4788544c937f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A long time ago in a galaxy far far away... a little girl named Lily was playing in the garden. She was so excited! She wanted to explore the garden and see what was around her.\n",
            "\n",
            "Suddenly, she heard a loud noise. Lily looked up and saw a big, hairy creature. Lily was so excited! She ran to the creature and grabbed it by the arm. The creature was so big and hairy that Lily couldn't help but laugh. \n",
            "\n",
            "Lily was so\n"
          ]
        }
      ]
    }
  ]
}